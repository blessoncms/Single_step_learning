{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "single_step_jump_to_wow.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6p2utNMKqRjG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dd41a619-b324-4465-b344-b5f91e3b48aa"
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "@author:Blesson George\n",
        "\"\"\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n@author:Blesson George\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehxocGoEqRjY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3e4a22e4-1aff-4c77-ef26-215689275005"
      },
      "source": [
        "# import necessary packages\n",
        "import tensorflow as tf\n",
        "import json\n",
        "from keras.models import Sequential\n",
        "from tf.keras.utils import np_utils\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.datasets import cifar10\n",
        "from keras import regularizers\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from keras.callbacks import EarlyStopping\n",
        "import numpy as np"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcjNLeZjqRjn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def balanced_sample_maker(X, y, sample_size, random_seed=42):\n",
        "    uniq_levels = np.unique(y)\n",
        "    uniq_counts = {level: sum(y == level) for level in uniq_levels}\n",
        "\n",
        "    if not random_seed is None:\n",
        "        np.random.seed(random_seed)\n",
        "\n",
        "    # find observation index of each class levels\n",
        "    groupby_levels = {}\n",
        "    for ii, level in enumerate(uniq_levels):\n",
        "        obs_idx = [idx for idx, val in enumerate(y) if val == level]\n",
        "        groupby_levels[level] = obs_idx\n",
        "    # oversampling on observations of each label\n",
        "    balanced_copy_idx = []\n",
        "    for gb_level, gb_idx in groupby_levels.items():\n",
        "        over_sample_idx = np.random.choice(gb_idx, size=sample_size, replace=True).tolist()\n",
        "        balanced_copy_idx+=over_sample_idx\n",
        "    np.random.shuffle(balanced_copy_idx)\n",
        "    data_train=X[balanced_copy_idx]\n",
        "    labels_train=y[balanced_copy_idx]\n",
        "    if  ((len(data_train)) == (sample_size*len(uniq_levels))):\n",
        "        print('number of sampled example ', sample_size*len(uniq_levels), 'number of sample per class ', sample_size, ' #classes: ', len(list(set(uniq_levels))))\n",
        "    else:\n",
        "        print('number of samples is wrong ')\n",
        "    print('data train',data_train.shape, labels_train)\t\n",
        "    return (data_train,labels_train,balanced_copy_idx)\n",
        "\n",
        "def next_picker(nX,ny,old_train,old_label,incorrect,prob):\n",
        "    x=nX[incorrect]\n",
        "    y=ny[incorrect]\n",
        "    uniq_levels=np.unique(y)\n",
        "    uniq_counts={lev:sum(y==lev) for lev in uniq_levels}\n",
        "    groupby_levels = {}\n",
        "    for ii, level in enumerate(uniq_levels):\n",
        "        obs_idx = [idx for idx, val in enumerate(y) if val == level]\n",
        "        groupby_levels[level] = obs_idx\n",
        "    # oversampling on observations of each label\n",
        "    balanced_copy_idx = []\n",
        "    for gb_level, gb_idx in groupby_levels.items():\n",
        "        #print(type(gb_idx),len(gb_idx),gb_idx)\n",
        "        #print([k for k in gb_idx])\n",
        "        #print(prob.keys())\n",
        "        prob_lev={k:prob[k] for k in gb_idx}\n",
        "        \n",
        "        maxx=max(prob_lev.values())\n",
        "        keys = [x for x,y in prob_lev.items() if y ==maxx]\n",
        "        if len(keys)>1:\n",
        "            keys=[(np.random.choice(np.array(keys)))]\n",
        "            \n",
        "        balanced_copy_idx+=keys\n",
        "    print('y',y)\n",
        "    print('balanced copy',balanced_copy_idx)\n",
        "    data_train=x[balanced_copy_idx]\n",
        "    labels_train=np.reshape(y[balanced_copy_idx],(10,1))\n",
        "    newX =nX[np.setdiff1d(np.arange(nX.shape[0]), balanced_copy_idx)]\n",
        "    newy = ny[np.setdiff1d(np.arange(ny.shape[0]), balanced_copy_idx)]\n",
        "    print('old train',old_train.shape,'data train',data_train.shape)\n",
        "    print('old label',old_label.shape,'labels train',labels_train.shape)\n",
        "    data_train=np.concatenate((old_train,data_train),axis=0)\n",
        "    labels_train=np.concatenate((old_label,labels_train),axis=0)\n",
        "    return (data_train,labels_train,newX,newy)\n",
        "\n",
        " \n",
        "def lr_schedule(epoch):\n",
        "    lrate = 0.001\n",
        "    if epoch > 75:\n",
        "        lrate = 0.0005\n",
        "    elif epoch > 100:\n",
        "        lrate = 0.0003        \n",
        "    return lrate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdX5EabFqRjr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "276e1b03-eb5d-4358-f64e-4c7c474ea957"
      },
      "source": [
        "batch_size=4\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "print(x_train.shape,x_test.shape,type(x_train),y_train.shape,y_test.shape)\n",
        "data=np.concatenate((x_train,x_test),axis=0)\n",
        "print('DATA',data.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "(50000, 32, 32, 3) (10000, 32, 32, 3) <class 'numpy.ndarray'> (50000, 1) (10000, 1)\n",
            "DATA (60000, 32, 32, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gRryHvaqRjy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "72111dfb-e9b2-4c2c-c828-fe5888a9444c"
      },
      "source": [
        "labels=np.concatenate((y_train,y_test),axis=0)\n",
        "#labels=labels[0:100]\n",
        "print('LABELS',labels.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LABELS (60000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfv38RV3qRj4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#z-score\n",
        "mean = np.mean(data,axis=(0,1,2,3))\n",
        "std = np.std(data,axis=(0,1,2,3))\n",
        "data=(data-mean)/std+1e-7"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9-OUkgtqRj8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d38969b0-3e0e-4ece-9a5c-8faee4348e01"
      },
      "source": [
        "num_classes = 10\n",
        "\n",
        "weight_decay = 1e-4\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=x_train.shape[1:]))\n",
        "model.add(tf.keras.layers.Activation('elu'))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(tf.keras.layers.Activation('elu'))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(tf.keras.layers.Activation('elu'))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(tf.keras.layers.Activation('elu'))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(tf.keras.layers.Dropout(0.3))\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(tf.keras.layers.Activation('elu'))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(tf.keras.layers.Activation('elu'))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(tf.keras.layers.Dropout(0.4))\n",
        "\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "#data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    )"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 32, 32, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 8, 8, 128)         147584    \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                20490     \n",
            "=================================================================\n",
            "Total params: 309,290\n",
            "Trainable params: 308,394\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_IqaEn6qRkE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "470febb9-de44-458e-dfb7-80959d1750bc"
      },
      "source": [
        "#Train trainset and labelset\n",
        "trainset,labelset,sample_idx=balanced_sample_maker(data,labels,1)\n",
        "newX = data[np.setdiff1d(np.arange(data.shape[0]), sample_idx)]\n",
        "newy = labels[np.setdiff1d(np.arange(labels.shape[0]), sample_idx)]\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of sampled example  10 number of sample per class  1  #classes:  10\n",
            "data train (10, 32, 32, 3) [[0]\n",
            " [6]\n",
            " [9]\n",
            " [1]\n",
            " [8]\n",
            " [5]\n",
            " [2]\n",
            " [3]\n",
            " [4]\n",
            " [7]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_XfOcU6sEPI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "69a03fcb-25b3-4ce1-b7ce-90df6c3bdbd8"
      },
      "source": [
        "import tensorflow as tf \n",
        "import os\n",
        "tpu_model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "    model,\n",
        "    strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "        tf.contrib.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "    )\n",
        ")\n",
        "\n",
        "tpu_model.compile(\n",
        "    optimizer=tf.train.RMSPropOptimizer(learning_rate=0.001,decay=1e-6),\n",
        "    loss=tf.keras.losses.categorical_crossentropy,\n",
        "    metrics=['accuracy']\n",
        ")\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0715 13:14:53.501694 140487790315392 keras_support.py:217] Keras support is now deprecated in support of TPU Strategy. Please follow the distribution strategy guide on tensorflow.org to migrate to the 2.0 supported version.\n",
            "W0715 13:15:02.273197 140487790315392 keras_support.py:1394] Keras support is now deprecated in support of TPU Strategy. Please follow the distribution strategy guide on tensorflow.org to migrate to the 2.0 supported version.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Y2MAq2_r-v5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "model.save_weights('model.h5')\n",
        "iter=0\n",
        "results={}\n",
        "for iter in range(0,1000):\n",
        "    print('Iteration No',iter,'starts')    \n",
        "    print('Trainset Shape',trainset.shape,'Trainlabels',labelset.shape)\n",
        "    datagen.fit(trainset)\n",
        "    label_one_hot=np_utils.to_categorical(labelset,num_classes)\n",
        "    model.fit_generator(datagen.flow(trainset, label_one_hot, batch_size=4),\\\n",
        "                    steps_per_epoch=trainset.shape[0] //batch_size,epochs=125,\\\n",
        "                    verbose=2,callbacks=[LearningRateScheduler(lr_schedule)])#,EarlyStopping(monitor='val_loss', patience=2, verbose=2,mode='auto',baseline=None)])#,restore_best_weights=True)])\n",
        "    #save to disk\n",
        "    #model_json = model.to_json()\n",
        "    #with open('model.json', 'w') as json_file:\n",
        "     #   json_file.write(model_json)\n",
        "\n",
        "    #model.save_weights('model.h5')  \n",
        "    ans=model.predict(newX,batch_size=32, verbose=2)\n",
        "    print('prediction',ans.shape,type(ans))  \n",
        "    maxlabel=np.argmax(ans,axis=1)\n",
        "    #maxlabel=np.array(maxlabel.reshape(maxlabel.shape[0],))\n",
        "    print('MaxLabel',maxlabel.shape)\n",
        "    maxval=np.max(ans,axis=1)\n",
        "    print('Maxval',maxval.shape)\n",
        "    newy=np.array(newy.reshape(newy.shape[0],))\n",
        "    print('newy',newy.shape)\n",
        "    print('maxval',maxval)\n",
        "    print('maxlabel',maxlabel)\n",
        "    print('newy',newy)\n",
        "\n",
        "    incorrect=np.where(maxlabel!=newy)\n",
        "    print(incorrect)\n",
        "    newyset=np_utils.to_categorical(newy,num_classes)\n",
        "    scores = model.evaluate(newX,newyset, batch_size=128, verbose=10)\n",
        "    #print('\\nIteration No:%i Test result: %.3f loss: %.3f' % (iter,scoresi[1]*100,scores[0]))\n",
        "    results[iter]=('Accuracy:',scores[1]*100,'Loss:',scores[0],'Training Size:',trainset.shape[0])\n",
        "    with open('resapr_30.json','w') as fp:\n",
        "        json.dump(results,fp)   \n",
        "    #    file.write(res)\n",
        "    trainset,labelset,newX,newy=next_picker(newX,newy,trainset,labelset,incorrect,maxval)\n",
        "    print('\\nIteration No:%i Test result: %.3f loss: %.3f' % (iter,scores[1]*100,scores[0]))\n",
        "    model.load_weights('model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LkGwGk3xi60",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -l -s git://github.com/jakevdp/PythonDataScienceHandbook.git cloned-repo\n",
        "%cd cloned-repo\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igL2f_B3xi5b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fetch a single <1MB file using the raw GitHub URL.\n",
        "!curl --remote-name \\\n",
        "     -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "     --location https://api.github.com/repos/jakevdp/PythonDataScienceHandbook/contents/notebooks/data/california_cities.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0RV4oq2sDAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9YKjKjeqRkJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "#    iter+=1\n",
        "    for i in range(0,newX.shape[0]):\n",
        "        if (np.argmax(model.predict(newX[i].reshape(1,32,32,3)),axis=1)!=newy[i]):\n",
        "\n",
        "            incorrect.append(i)\n",
        "        prob_list[i]=(np.max(model.predict(newX[i].reshape(1,32,32,3)),axis=1))\n",
        "    print('Total No.',newX.shape[0],'InCorrect Predictions',len(incorrect))    \n",
        "   ''' \n",
        "    newyset=np_utils.to_categorical(newy,num_classes)\n",
        "    scores = model.evaluate(newX,newyset, batch_size=128, verbose=10)\n",
        "    #print('\\nIteration No:%i Test result: %.3f loss: %.3f' % (iter,scoresi[1]*100,scores[0]))\n",
        "    results[iter]=('Accuracy:',scores[1]*100,'Loss:',scores[0],'Training Size:',trainset.shape[0])\n",
        "    with open('resapr_26.json','w') as fp:\n",
        "        json.dump(results,fp)   \n",
        "    #    file.write(res)\n",
        "    trainset,labelset,newX,newy=next_picker(newX,newy,trainset,labelset,incorrect,maxval)\n",
        "    print('\\nIteration No:%i Test result: %.3f loss: %.3f' % (iter,scores[1]*100,scores[0]))\n",
        "    model.load_weights('model.h5')\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}