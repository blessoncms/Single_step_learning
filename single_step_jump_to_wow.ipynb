{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "@author:Blesson George\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d85668a64f21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \"\"\"\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "# import necessary packages\n",
    "import keras,json\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.datasets import cifar10\n",
    "from keras import regularizers\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.callbacks import EarlyStopping\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_sample_maker(X, y, sample_size, random_seed=42):\n",
    "    uniq_levels = np.unique(y)\n",
    "    uniq_counts = {level: sum(y == level) for level in uniq_levels}\n",
    "\n",
    "    if not random_seed is None:\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "    # find observation index of each class levels\n",
    "    groupby_levels = {}\n",
    "    for ii, level in enumerate(uniq_levels):\n",
    "        obs_idx = [idx for idx, val in enumerate(y) if val == level]\n",
    "        groupby_levels[level] = obs_idx\n",
    "    # oversampling on observations of each label\n",
    "    balanced_copy_idx = []\n",
    "    for gb_level, gb_idx in groupby_levels.items():\n",
    "        over_sample_idx = np.random.choice(gb_idx, size=sample_size, replace=True).tolist()\n",
    "        balanced_copy_idx+=over_sample_idx\n",
    "    np.random.shuffle(balanced_copy_idx)\n",
    "    data_train=X[balanced_copy_idx]\n",
    "    labels_train=y[balanced_copy_idx]\n",
    "    if  ((len(data_train)) == (sample_size*len(uniq_levels))):\n",
    "        print('number of sampled example ', sample_size*len(uniq_levels), 'number of sample per class ', sample_size, ' #classes: ', len(list(set(uniq_levels))))\n",
    "    else:\n",
    "        print('number of samples is wrong ')\n",
    "    print('data train',data_train.shape, labels_train)\t\n",
    "    return (data_train,labels_train,balanced_copy_idx)\n",
    "\n",
    "def next_picker(nX,ny,old_train,old_label,incorrect,prob):\n",
    "    x=nX[incorrect]\n",
    "    y=ny[incorrect]\n",
    "    uniq_levels=np.unique(y)\n",
    "    uniq_counts={lev:sum(y==lev) for lev in uniq_levels}\n",
    "    groupby_levels = {}\n",
    "    for ii, level in enumerate(uniq_levels):\n",
    "        obs_idx = [idx for idx, val in enumerate(y) if val == level]\n",
    "        groupby_levels[level] = obs_idx\n",
    "    # oversampling on observations of each label\n",
    "    balanced_copy_idx = []\n",
    "    for gb_level, gb_idx in groupby_levels.items():\n",
    "        #print(type(gb_idx),len(gb_idx),gb_idx)\n",
    "        #print([k for k in gb_idx])\n",
    "        #print(prob.keys())\n",
    "        prob_lev={k:prob[k] for k in gb_idx}\n",
    "        \n",
    "        maxx=max(prob_lev.values())\n",
    "        keys = [x for x,y in prob_lev.items() if y ==maxx]\n",
    "        if len(keys)>1:\n",
    "            keys=[(np.random.choice(np.array(keys)))]\n",
    "            \n",
    "        balanced_copy_idx+=keys\n",
    "    print('y',y)\n",
    "    print('balanced copy',balanced_copy_idx)\n",
    "    data_train=x[balanced_copy_idx]\n",
    "    labels_train=np.reshape(y[balanced_copy_idx],(10,1))\n",
    "    newX =nX[np.setdiff1d(np.arange(nX.shape[0]), balanced_copy_idx)]\n",
    "    newy = ny[np.setdiff1d(np.arange(ny.shape[0]), balanced_copy_idx)]\n",
    "    print('old train',old_train.shape,'data train',data_train.shape)\n",
    "    print('old label',old_label.shape,'labels train',labels_train.shape)\n",
    "    data_train=np.concatenate((old_train,data_train),axis=0)\n",
    "    labels_train=np.concatenate((old_label,labels_train),axis=0)\n",
    "    return (data_train,labels_train,newX,newy)\n",
    "\n",
    " \n",
    "def lr_schedule(epoch):\n",
    "    lrate = 0.001\n",
    "    if epoch > 75:\n",
    "        lrate = 0.0005\n",
    "    elif epoch > 100:\n",
    "        lrate = 0.0003        \n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=4\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "print(x_train.shape,x_test.shape,type(x_train),y_train.shape,y_test.shape)\n",
    "data=np.concatenate((x_train,x_test),axis=0)\n",
    "print('DATA',data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=np.concatenate((y_train,y_test),axis=0)\n",
    "#labels=labels[0:100]\n",
    "print('LABELS',labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#z-score\n",
    "mean = np.mean(data,axis=(0,1,2,3))\n",
    "std = np.std(data,axis=(0,1,2,3))\n",
    "data=(data-mean)/std+1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "\n",
    "weight_decay = 1e-4\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(-1.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "#data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train trainset and labelset\n",
    "trainset,labelset,sample_idx=balanced_sample_maker(data,labels,1)\n",
    "newX = data[np.setdiff1d(np.arange(data.shape[0]), sample_idx)]\n",
    "newy = labels[np.setdiff1d(np.arange(labels.shape[0]), sample_idx)]\n",
    "opt_rms = keras.optimizers.rmsprop(lr=0.001,decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
    "model.save_weights('model.h5')\n",
    "iter=0\n",
    "results={}\n",
    "for iter in range(0,1000):\n",
    "    print('Iteration No',iter,'starts')    \n",
    "    print('Trainset Shape',trainset.shape,'Trainlabels',labelset.shape)\n",
    "    datagen.fit(trainset)\n",
    "    label_one_hot=np_utils.to_categorical(labelset,num_classes)\n",
    "    model.fit_generator(datagen.flow(trainset, label_one_hot, batch_size=4),\\\n",
    "                    steps_per_epoch=trainset.shape[0] //batch_size,epochs=125,\\\n",
    "                    verbose=2,callbacks=[LearningRateScheduler(lr_schedule)])#,EarlyStopping(monitor='val_loss', patience=2, verbose=2,mode='auto',baseline=None)])#,restore_best_weights=True)])\n",
    "    #save to disk\n",
    "    #model_json = model.to_json()\n",
    "    #with open('model.json', 'w') as json_file:\n",
    "     #   json_file.write(model_json)\n",
    "\n",
    "    #model.save_weights('model.h5')  \n",
    "    ans=model.predict(newX,batch_size=32, verbose=2)\n",
    "    print('prediction',ans.shape,type(ans))  \n",
    "    maxlabel=np.argmax(ans,axis=1)\n",
    "    #maxlabel=np.array(maxlabel.reshape(maxlabel.shape[0],))\n",
    "    print('MaxLabel',maxlabel.shape)\n",
    "    maxval=np.max(ans,axis=1)\n",
    "    print('Maxval',maxval.shape)\n",
    "    newy=np.array(newy.reshape(newy.shape[0],))\n",
    "    print('newy',newy.shape)\n",
    "    print('maxval',maxval)\n",
    "    print('maxlabel',maxlabel)\n",
    "    print('newy',newy)\n",
    "\n",
    "    incorrect=np.where(maxlabel!=newy)\n",
    "    print(incorrect)\n",
    "    newyset=np_utils.to_categorical(newy,num_classes)\n",
    "    scores = model.evaluate(newX,newyset, batch_size=128, verbose=10)\n",
    "    #print('\\nIteration No:%i Test result: %.3f loss: %.3f' % (iter,scoresi[1]*100,scores[0]))\n",
    "    results[iter]=('Accuracy:',scores[1]*100,'Loss:',scores[0],'Training Size:',trainset.shape[0])\n",
    "    with open('resapr_30.json','w') as fp:\n",
    "        json.dump(results,fp)   \n",
    "    #    file.write(res)\n",
    "    trainset,labelset,newX,newy=next_picker(newX,newy,trainset,labelset,incorrect,maxval)\n",
    "    print('\\nIteration No:%i Test result: %.3f loss: %.3f' % (iter,scores[1]*100,scores[0]))\n",
    "    model.load_weights('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#    iter+=1\n",
    "    for i in range(0,newX.shape[0]):\n",
    "        if (np.argmax(model.predict(newX[i].reshape(1,32,32,3)),axis=1)!=newy[i]):\n",
    "\n",
    "            incorrect.append(i)\n",
    "        prob_list[i]=(np.max(model.predict(newX[i].reshape(1,32,32,3)),axis=1))\n",
    "    print('Total No.',newX.shape[0],'InCorrect Predictions',len(incorrect))    \n",
    "   ''' \n",
    "    newyset=np_utils.to_categorical(newy,num_classes)\n",
    "    scores = model.evaluate(newX,newyset, batch_size=128, verbose=10)\n",
    "    #print('\\nIteration No:%i Test result: %.3f loss: %.3f' % (iter,scoresi[1]*100,scores[0]))\n",
    "    results[iter]=('Accuracy:',scores[1]*100,'Loss:',scores[0],'Training Size:',trainset.shape[0])\n",
    "    with open('resapr_26.json','w') as fp:\n",
    "        json.dump(results,fp)   \n",
    "    #    file.write(res)\n",
    "    trainset,labelset,newX,newy=next_picker(newX,newy,trainset,labelset,incorrect,maxval)\n",
    "    print('\\nIteration No:%i Test result: %.3f loss: %.3f' % (iter,scores[1]*100,scores[0]))\n",
    "    model.load_weights('model.h5')\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
